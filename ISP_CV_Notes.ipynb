{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#3D-Imaging\" data-toc-modified-id=\"3D-Imaging-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>3D Imaging</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stereo-camera-&amp;-Disparity-map\" data-toc-modified-id=\"Stereo-camera-&amp;-Disparity-map-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Stereo camera &amp; Disparity map</a></span></li><li><span><a href=\"#Depth-map-with-Kinect-camera\" data-toc-modified-id=\"Depth-map-with-Kinect-camera-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Depth map with Kinect camera</a></span></li></ul></li><li><span><a href=\"#Object-detection\" data-toc-modified-id=\"Object-detection-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Object detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Strided-convolution-for-object-detection\" data-toc-modified-id=\"Strided-convolution-for-object-detection-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Strided convolution for object detection</a></span></li><li><span><a href=\"#R-CNN-for-object-detection\" data-toc-modified-id=\"R-CNN-for-object-detection-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>R-CNN for object detection</a></span></li><li><span><a href=\"#YOLO-(You-Only-Look-Once)-real-time-object-detection\" data-toc-modified-id=\"YOLO-(You-Only-Look-Once)-real-time-object-detection-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>YOLO (You Only Look Once) real time object detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#IOU-(Intersection-Over-Union)-to-evaluate-bounding-box\" data-toc-modified-id=\"IOU-(Intersection-Over-Union)-to-evaluate-bounding-box-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>IOU (Intersection Over Union) to evaluate bounding box</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-IOU-=-\\frac{Intersection-of-pixels-between-2-pixels}{total-union-pixels-of-2-boxes}-$-\" data-toc-modified-id=\"-$-IOU-=-\\frac{Intersection-of-pixels-between-2-pixels}{total-union-pixels-of-2-boxes}-$--2.3.1.1\"><span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span> $ IOU = \\frac{Intersection of pixels between 2 pixels}{total union pixels of 2 boxes} $ </a></span></li></ul></li><li><span><a href=\"#Non-max-suppression\" data-toc-modified-id=\"Non-max-suppression-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Non-max suppression</a></span></li><li><span><a href=\"#Anchor-boxes\" data-toc-modified-id=\"Anchor-boxes-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Anchor boxes</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo camera & Disparity map\n",
    "- To perceive the depth, human visual system has 2 eyes to simulate 3D world environment\n",
    "- Using 2 cameras (known as stereo setup) at a certain distance we can simulate the 3D world with depth\n",
    "- The images seen by 2 cameras will be slightly different, the difference between the 2 images known as disparity map which indicates the depth\n",
    "\n",
    "<img src=\"images/stereo_images_disparity.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth map with Kinect camera\n",
    "- Kinect camera projects a known pattern on to the scene and infers depth from the the pattern\n",
    "- Kinect combines structure light with 2 computer vision techniques 1) depth from focus 2) depth from stereo\n",
    "- Kinect also infers the body position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strided convolution for object detection\n",
    "- Approaches to get regions for object detection is, get each region with a stride and classify with a CNN, but this is computationally expensive, \n",
    "- A better approach is to use strided convolution to compute for all possible regions with a single forward pass, but the accuracy is less due to the objects may not fit in to the size of fixed region sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN for object detection\n",
    "- [Tutorial](https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/)\n",
    "- The R-CNN solves above problems with a series of steps,\n",
    "    - **divide the image into various regions ~2000 (with segmentation)**, then consider each region as a separate image\n",
    "    - **Pass all these regions (images) to the CNN and classify them into various classes**\n",
    "    - Combine the classes of the regions to form objects in full image\n",
    "- Select regions with a segmentation and combines the similar regions to form a larger region (based on color similarity, texture similarity, size similarity, and shape compatibility). Finally, these regions then produce the final object locations (Region of Interest)\n",
    "- All the regions are then reshaped as per the input of the CNN, and each region is passed to the ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO (You Only Look Once) real time object detection\n",
    "- [Tutorial](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/)\n",
    "- Its a very good, popular algorithm with real time speed\n",
    "- In this image is divided in to some grids (say 19x19), then it goes to convnet for classification and localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU (Intersection Over Union) to evaluate bounding box\n",
    "- To evaluate the accuracy of predicted bounding box, we will find total pixels covered by both predicted and label boxes, then find ratio of (intersection/union)\n",
    "<h4 align=\"center\"> $ IOU = \\frac{Intersection of pixels between 2 pixels}{total union pixels of 2 boxes} $ </h4>\n",
    "- Typically IOU >= 0.5 is a reasonable prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-max suppression\n",
    "- Non-max suppression removes multiple detected boxes for an object, by selecting highest probability one suppressing other boxes which has high IOU with the highest predicted box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
