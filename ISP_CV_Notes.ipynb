{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Topics-to-learn\" data-toc-modified-id=\"Topics-to-learn-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Topics to learn</a></span></li><li><span><a href=\"#Continuous,-Discrete-Signals-\" data-toc-modified-id=\"Continuous,-Discrete-Signals--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"DarkOrange\">Continuous, Discrete Signals </font></a></span></li><li><span><a href=\"#-3D-Imaging-\" data-toc-modified-id=\"-3D-Imaging--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"DarkOrange\"> 3D Imaging </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Stereo-camera-&amp;-Disparity-map\" data-toc-modified-id=\"Stereo-camera-&amp;-Disparity-map-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Stereo camera &amp; Disparity map</a></span></li><li><span><a href=\"#Depth-map-with-Kinect-camera\" data-toc-modified-id=\"Depth-map-with-Kinect-camera-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Depth map with Kinect camera</a></span></li></ul></li><li><span><a href=\"#-Depth-Estimation-\" data-toc-modified-id=\"-Depth-Estimation--4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Depth Estimation </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Hardware\" data-toc-modified-id=\"Hardware-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Hardware</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dual-camera-technology\" data-toc-modified-id=\"Dual-camera-technology-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Dual camera technology</a></span></li><li><span><a href=\"#Dual-pixel-technology\" data-toc-modified-id=\"Dual-pixel-technology-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Dual pixel technology</a></span></li><li><span><a href=\"#-Sensors:-Depth-Cameras-\" data-toc-modified-id=\"-Sensors:-Depth-Cameras--4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Sensors: Depth Cameras </font></a></span></li></ul></li><li><span><a href=\"#Software\" data-toc-modified-id=\"Software-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Software</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-image-methods\" data-toc-modified-id=\"Multiple-image-methods-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Multiple image methods</a></span></li><li><span><a href=\"#Single-image-methods:-Supervised-Learning-based-methods\" data-toc-modified-id=\"Single-image-methods:-Supervised-Learning-based-methods-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Single image methods: Supervised Learning based methods</a></span></li><li><span><a href=\"#Single-image-methods:-Unsupervised-Learning-based-methods\" data-toc-modified-id=\"Single-image-methods:-Unsupervised-Learning-based-methods-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Single image methods: Unsupervised Learning based methods</a></span></li></ul></li></ul></li><li><span><a href=\"#-Object-detection-\" data-toc-modified-id=\"-Object-detection--5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Object detection </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Strided-convolution-for-object-detection\" data-toc-modified-id=\"Strided-convolution-for-object-detection-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Strided convolution for object detection</a></span></li><li><span><a href=\"#R-CNN-for-object-detection\" data-toc-modified-id=\"R-CNN-for-object-detection-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>R-CNN for object detection</a></span></li><li><span><a href=\"#YOLO-(You-Only-Look-Once)-real-time-object-detection\" data-toc-modified-id=\"YOLO-(You-Only-Look-Once)-real-time-object-detection-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>YOLO (You Only Look Once) real time object detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#IOU-(Intersection-Over-Union)-to-evaluate-bounding-box\" data-toc-modified-id=\"IOU-(Intersection-Over-Union)-to-evaluate-bounding-box-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>IOU (Intersection Over Union) to evaluate bounding box</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-IOU-=-\\frac{Intersection-of-pixels-between-2-pixels}{total-union-pixels-of-2-boxes}-$-\" data-toc-modified-id=\"-$-IOU-=-\\frac{Intersection-of-pixels-between-2-pixels}{total-union-pixels-of-2-boxes}-$--5.3.1.1\"><span class=\"toc-item-num\">5.3.1.1&nbsp;&nbsp;</span> $ IOU = \\frac{Intersection of pixels between 2 pixels}{total union pixels of 2 boxes} $ </a></span></li></ul></li><li><span><a href=\"#Non-max-suppression\" data-toc-modified-id=\"Non-max-suppression-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Non-max suppression</a></span></li><li><span><a href=\"#Anchor-boxes\" data-toc-modified-id=\"Anchor-boxes-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Anchor boxes</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics to learn\n",
    "- Traingulation\n",
    "- Point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange>Continuous, Discrete Signals </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> 3D Imaging </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo camera & Disparity map\n",
    "- To perceive the depth, human visual system has 2 eyes to simulate 3D world environment\n",
    "- Using 2 cameras (known as stereo setup) at a certain distance we can simulate the 3D world with depth\n",
    "- The images seen by 2 cameras will be slightly different, <font color=blue> the difference between the 2 images known as disparity map which indicates the depth </font>\n",
    "\n",
    "<img src=\"images/stereo_images_disparity.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth map with Kinect camera\n",
    "- Kinect camera projects a known pattern on to the scene and infers depth from the the pattern\n",
    "- Kinect combines structure light with 2 computer vision techniques 1) depth from focus 2) depth from stereo\n",
    "- Kinect also infers the body position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Depth Estimation </font>\n",
    "- [Depth Estimation Tutorial](https://medium.com/beyondminds/depth-estimation-cad24b0099f)\n",
    "- Our eyes estimate depth by comparing the image obtained by our left and right eye. The minor displacement between both viewpoints is enough to calculate an approximate depth map. We call the pair of images obtained by our eyes a stereo pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual camera technology\n",
    "- Some devices have two cameras separated by a small distance (usually a few millimeters) to capture images from different viewpoints. These two images form a stereo pair, and is used to compute depth information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual pixel technology\n",
    "- An alternative solution to the Dual Camera technology is Dual Pixel Autofocus (DPAF) technology.\n",
    "- Each pixel is comprised of two photodiodes, which are separated by a very small distance (less than a millimeter). Each photodiode considers the image signals separately, and then analyzes it. \n",
    "- Popularly, Google Pixel 2 uses this technology to calculate depth information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> Sensors: Depth Cameras </font>\n",
    "- [Depth Camera Tutorial](https://medium.com/@DAQRI/depth-cameras-for-mobile-ar-from-iphones-to-wearables-and-beyond-ea29758ec280)\n",
    "- A good alternative to multiple cameras is to use sensors that can infer distance. For instance, the first version of Kinect used an Infra-Red (IR) projector to achieve this. \n",
    "- A pattern of IR dots is projected on to the environment, and a monochrome CMOS sensor (placed a few centimeters apart) received the reflected rays. The difference between the expected and received IR dot positions is calculated to produce the depth information.\n",
    "- As with any sensor, depth cameras need to be calibrated to function properly.\n",
    "- Both structured light and active stereo, as well as passive stereo, are all stereo-based solutions — meaning that depth estimations are based on identifying features from two different viewpoints so that they can be triangulated.\n",
    "- Note that the depth estimation is stereo because of the two viewpoints needed to estimate depth, not the number of cameras\n",
    "- **Only Time of Flight does not rely on stereo principles and measures depth directly**\n",
    "\n",
    "<img src=\"images/depth_cameras.PNG\" width=\"700\">\n",
    "\n",
    "- ** Structured Light **\n",
    "- <font color=blue> As mentioned above, structured light (SL) projects a known pattern using a projector. The projector and camera form the stereo system, hence no second camera is required. </font>\n",
    "- SL systems usually work in the infrared spectrum so that the pattern is not visible to the human eye.\n",
    "- For safety reasons, the projector needs to be limited in projection power, usually resulting in a projection distance of up to 4–5 meters. \n",
    "- As with all stereo-based solutions, structured light can suffer from structural integrity. in order to achieve high quality measurements up to 5m, a baseline of ~8 cm is required. \n",
    "- To summarize, structured light doesn’t measure depth directly, but relies on stereo matching\n",
    "- --\n",
    "- ** ACTIVE AND PASSIVE STEREO **\n",
    "- <font color=blue> Active and passive stereo are quite similar in that they both rely on matching features from one camera to another. </font>\n",
    "- Passive stereo suffers from not being able to match uniformly colored surfaces such as white walls or empty desks that simply lack features, such as texture, to match.\n",
    "- Active stereo overcomes this problem by projecting a random pattern onto the environment such that even texture less surfaces can be matched between the two views.\n",
    "- all stereo-based solutions suffer from shadowing effects, which arise from the one view not observing exactly the same part of the scene as the other view: At large distances both cameras see mostly the same, but close up the two cameras can see very different parts of objects.\n",
    "- --\n",
    "- **TIME OF FLIGHT (TOF)**\n",
    "- While stereo-based solutions estimate depth by triangulating matched features, <font color=blue> Time of Flight (ToF) cameras measure distance directly, meaning that the device measures the time offset between a signal sent out by the emitter until it arrives back at the sensor. </font>\n",
    "- Based on the measured time offset and using the known speed of light, distance can be calculated. However, due to the vast speed of light, sub-nanosecond timing accuracy is required to get accurate results.\n",
    "- Due to technical challenges, only a **few devices actually use this principle: one well-known device class are LIDAR sensors,** which are popular in self-driving car research today.\n",
    "- **Most ToF cameras used in AR and VR today are not measuring the time of the signal, but its phase shift instead, from which distance can also be estimated **\n",
    "- ToF cameras today typically have a relatively low resolution of 100k pixels and often even lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple image methods\n",
    "- The easiest way to calculate depth information without using additional hardware is to take multiple images of the same scene with slight displacements.\n",
    "- By matching keypoints that are common with each image, we can reconstruct a 3D model of the scene. Algorithms such as Scale-Invariant Feature Transform (SIFT) are excellent at this task.\n",
    "- To make this method more robust, we can measure the change in orientation of the device to calculate the physical distance between the two images. This can be done by measuring the accelerometer and gyroscope data of the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image methods: Supervised Learning based methods\n",
    "- Supervised methods require some sort of labels to be trained. Usually, the labels are pixel-wise RGB-D depth maps. In such cases, the trained model can directly output the depth map. Commonly used depth datasets include the NYUv2 dataset\n",
    "- Target labels need not necessarily be pure depth maps, but can also be a function of depth maps, such as hazy images.\n",
    "- Autoencoder are among the simplest type of networks used to extract depth information. Popular variants involve using U-Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image methods: Unsupervised Learning based methods\n",
    "- It is hard to obtain depth datasets of high quality that account for all possible background conditions.\n",
    "- Some methods involves generating the right image, for a given left image in a stereo image pair, we calculate the disparity between the two images, which in our case is the displacement of a pixel (or block) in the right-image with respect to its location in the left-image\n",
    "- The limitation of using learning based methods, especially that of supervised methods, is that they may not generalize well to all use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Object detection </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strided convolution for object detection\n",
    "- Approaches to get regions for object detection is, get each region with a stride and classify with a CNN, but this is computationally expensive, \n",
    "- A better approach is to use strided convolution to compute for all possible regions with a single forward pass, but the accuracy is less due to the objects may not fit in to the size of fixed region sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN for object detection\n",
    "- [Tutorial](https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/)\n",
    "- The R-CNN solves above problems with a series of steps,\n",
    "    - **divide the image into various regions ~2000 (with segmentation)**, then consider each region as a separate image\n",
    "    - **Pass all these regions (images) to the CNN and classify them into various classes**\n",
    "    - Combine the classes of the regions to form objects in full image\n",
    "- Select regions with a segmentation and combines the similar regions to form a larger region (based on color similarity, texture similarity, size similarity, and shape compatibility). Finally, these regions then produce the final object locations (Region of Interest)\n",
    "- All the regions are then reshaped as per the input of the CNN, and each region is passed to the ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO (You Only Look Once) real time object detection\n",
    "- [Tutorial](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/)\n",
    "- Its a very good, popular algorithm with real time speed\n",
    "- In this image is divided in to some grids (say 19x19), then it goes to convnet for classification and localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU (Intersection Over Union) to evaluate bounding box\n",
    "- To evaluate the accuracy of predicted bounding box, we will find total pixels covered by both predicted and label boxes, then find ratio of (intersection/union)\n",
    "<h4 align=\"center\"> $ IOU = \\frac{Intersection of pixels between 2 pixels}{total union pixels of 2 boxes} $ </h4>\n",
    "- Typically IOU >= 0.5 is a reasonable prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-max suppression\n",
    "- Non-max suppression removes multiple detected boxes for an object, by selecting highest probability one suppressing other boxes which has high IOU with the highest predicted box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
